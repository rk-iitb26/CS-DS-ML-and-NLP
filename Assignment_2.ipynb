{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b761ca3",
   "metadata": {},
   "source": [
    "# Ques 1: SPAM HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17778547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nbformat\n",
    "import random\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "import contractions\n",
    "nltk.download('punkt_tab', quiet=True) #NLTK <‚ÄØ3.9 uses punkt\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True) \n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ae4b2",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\1 All about placements\\LS\\spam.csv', encoding='latin1') \n",
    "df.drop(columns=['Unnamed: 2', 'Unnamed: 3' , 'Unnamed: 4'], axis=1, inplace=True)\n",
    "df.rename(columns={'v1': 'label' , 'v2': 'text'}, inplace=True)\n",
    "df=df[['text', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopw word + to lower + tokenize\n",
    "\n",
    "df['tokens'] = df['text'].apply(lambda s: word_tokenize(s))\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(lambda ts: [t.lower() for t in ts])\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(\n",
    "    lambda ts: [t for t in ts if t not in stop_words]\n",
    ")\n",
    "\n",
    "sentences = df['tokens'].tolist() #not used in this program, useful for own w2v prep.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68558a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE !!!\n",
    "\n",
    "# model = Word2Vec(\n",
    "#     sentences, \n",
    "#     vector_size=200,   \n",
    "#     window=5,        \n",
    "#     min_count=1,    \n",
    "#     workers=4,\n",
    "#     seed=42    \n",
    "# )\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# X = pca.fit_transform(model.wv.get_normed_vectors())\n",
    "\n",
    "# labels = model.wv.index_to_key           \n",
    "# fig = px.scatter_3d(\n",
    "#     x=X[200:300, 0],\n",
    "#     y=X[200:300, 1],\n",
    "#     z=X[200:300, 2],\n",
    "#     color=labels[200:300],\n",
    "#     hover_name=labels[200:300]   \n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9effb",
   "metadata": {},
   "source": [
    "Transforming w2v google news on preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all tokens into a single list\n",
    "all_tokens = [word for message in df['tokens'] for word in message]\n",
    "\n",
    "# Count word frequencies\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Get Word2Vec vocab\n",
    "vocab = set(model.key_to_index)\n",
    "\n",
    "# In-vocab and out-of-vocab token counts\n",
    "in_vocab = {word: count for word, count in token_counts.items() if word in vocab}\n",
    "oov = {word: count for word, count in token_counts.items() if word not in vocab}\n",
    "\n",
    "# Function to convert a message to a 300-dim vector\n",
    "def message_to_vector(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model.key_to_index]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Apply the function to all rows\n",
    "df['vector'] = df['tokens'].apply(lambda tokens: message_to_vector(tokens, model))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88b3cd",
   "metadata": {},
   "source": [
    "Training and plotting accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(df['vector'].values)  \n",
    "y = df['label'].map({'ham': 0, 'spam': 1}).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in 1D for viz.\n",
    "pca = PCA(n_components=1)\n",
    "X1D = pca.fit_transform(X_test)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Sort for smooth line plot\n",
    "sorted_idx = np.argsort(X1D.flatten())\n",
    "X1D_sorted = X1D.flatten()[sorted_idx]\n",
    "y_prob_sorted = y_prob[sorted_idx]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(X1D_sorted, y_prob_sorted, label=\"Predicted Probability (Spam)\", color=\"red\")\n",
    "plt.scatter(X1D, y_test, alpha=0.4, label=\"True Labels (0=ham, 1=spam)\", s=10)\n",
    "plt.xlabel(\"PCA-reduced Feature\")\n",
    "plt.ylabel(\"Probability / Label\")\n",
    "plt.title(\"Logistic Regression Prediction vs Feature (1D Projection)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8eabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Ham', 'Spam'])\n",
    "disp.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"LogReg (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c61bb",
   "metadata": {},
   "source": [
    "predict_message_class > for preprocessing unkown text and it's prediction( following pipeline of model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message_class(model, w2v_model, message):\n",
    "    # Preprocess\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(message.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    # Get vectors\n",
    "    vectors = [w2v_model[word] for word in tokens if word in w2v_model.key_to_index]\n",
    "    \n",
    "    if not vectors:\n",
    "        message_vec = np.zeros(w2v_model.vector_size).reshape(1, -1)\n",
    "    else:\n",
    "        message_vec = np.mean(vectors, axis=0).reshape(1, -1)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(message_vec)[0]\n",
    "    return \"spam\" if pred == 1 else \"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unkown text and prediction\n",
    "message = [\"Congratulations! In lucky draw, you won a Iphone, claim now.\",\n",
    "            \"Hey, just checking if we‚Äôre still on for the project meeting tomorrow at 10 AM. Let me know if the timing works for you.\"]\n",
    "\n",
    "for i in message:\n",
    "    print(f'message: {i}, \\nOutput: {predict_message_class(clf, model, i)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7594617",
   "metadata": {},
   "source": [
    "# Ques 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b403a1",
   "metadata": {},
   "source": [
    "Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2672b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\1 All about placements\\LS\\Tweets.csv')\n",
    "df = df[['text', 'airline_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9b1c5",
   "metadata": {},
   "source": [
    "text data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "df['text'] = df['text'].apply(contractions.fix)\n",
    "df['text'] = df['text'].str.replace(r\"http\\S+|www\\S+|https\\S+\", '', regex=True)  # url links\n",
    "df['text'] = df['text'].str.replace(r\"@\\w+\", '', regex=True)                     # @mentions\n",
    "df['text'] = df['text'].str.replace(r\"#\", '', regex=True)                        # Hashtag symbol\n",
    "df['text'] = df['text'].str.replace(r\"[^\\w\\s]\", '', regex=True)                  # remove all unnecessary special symbols(including # and puntuation) keep text\n",
    "df['text'] = df['text'].str.lower()                                              # to lower\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))         # emoji\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc809f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all tokens into a single list\n",
    "all_tokens = [word for message in df['text'] for word in message]\n",
    "\n",
    "# Count word frequencies\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Get Word2Vec vocab\n",
    "vocab = set(model.key_to_index)\n",
    "\n",
    "# In-vocab and out-of-vocab token counts\n",
    "in_vocab = {word: count for word, count in token_counts.items() if word in vocab}\n",
    "oov = {word: count for word, count in token_counts.items() if word not in vocab}\n",
    "\n",
    "# Function to convert a message to a 300-dim vector\n",
    "def message_to_vector(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model.key_to_index]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Apply the function to all rows\n",
    "df['vector'] = df['text'].apply(lambda tokens: message_to_vector(tokens, model))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5923e",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(df['vector'].values)  \n",
    "y = df['airline_sentiment'].map({'positive': 2, 'neutral': 1, 'negative': 0}).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    multi_class='multinomial', \n",
    "    solver='newton-cg', \n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Neutral', 'Positive'])\n",
    "disp.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea31a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "n_classes = y_test_bin.shape[1]\n",
    "y_score = clf.predict_proba(X_test)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves (One-vs-Rest)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d8bc9",
   "metadata": {},
   "source": [
    "prediction function using preprocessing pipleine and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58416397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweet = contractions.fix(tweet)\n",
    "\n",
    "    # Remove URLs, mentions, hashtags, punctuation, emojis\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet)\n",
    "    tweet = re.sub(r\"@\\w+\", '', tweet)\n",
    "    tweet = re.sub(r\"#\", '', tweet)\n",
    "    tweet = re.sub(r\"[^\\w\\s]\", '', tweet)\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
    "\n",
    "    # Lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Tokenize, remove stopwords, lemmatize\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28154728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
    "    tokens = preprocess_tweet(tweet)\n",
    "    \n",
    "    # Average Word2Vec vector\n",
    "    vectors = [w2v_model[word] for word in tokens if word in w2v_model]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return \"unknown\"  # or neutral as fallback\n",
    "\n",
    "    avg_vector = np.mean(vectors, axis=0).reshape(1, -1)\n",
    "\n",
    "    pred = model.predict(avg_vector)[0]\n",
    "    label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    return label_map[pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\"@UnitedAirlines lost my baggage again... unbelievable service üò° #neveragain #airlinemess\",\n",
    "            \"Huge shoutout to @Delta for the smooth flight but irritating staff! ‚úàÔ∏è #ThankYou #FlyDelta\",\n",
    "            \"Boarding started on time at gate 24. Flight to NYC with @AmericanAir #TravelUpdate\"]\n",
    "\n",
    "for i in message:\n",
    "    print(f'message: {i}, \\nOutput: {predict_tweet_sentiment(clf, model, i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79addadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
