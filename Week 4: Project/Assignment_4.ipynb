{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d9643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf928a8",
   "metadata": {},
   "source": [
    "# Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0413cc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# loading dataset and tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "wiki = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\",\n",
    "    split={\n",
    "        \"train\": \"train[:15000]\",\n",
    "        \"validation\": \"validation[:3200]\",\n",
    "        \"test\": \"test[:4000]\"\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token    #padding\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7088934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 15000\n",
      "}) Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 3200\n",
      "}) Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 4000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# tokenizing and splitting data\n",
    "def preprocess(example):\n",
    "    tokens = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"]\n",
    "    return tokens\n",
    "\n",
    "wiki_tokenized = wiki.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "wiki_tokenized.set_format(\"torch\")\n",
    "\n",
    "train_dataset = wiki_tokenized[\"train\"]\n",
    "eval_dataset = wiki_tokenized[\"validation\"]\n",
    "test_dataset = wiki_tokenized[\"test\"]\n",
    "\n",
    "print(train_dataset, eval_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01d110",
   "metadata": {},
   "source": [
    "# Warm up time check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33af4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time ~150 min. \n",
      "Commented and restarted kernel to save CUDA memory \n",
      "(may continue without commenintg for GPU: min 8GB, RTX 3050)\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from transformers import GPT2LMHeadModel, TrainingArguments, EarlyStoppingCallback, Trainer\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # warmup time \n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./gpt2-finetuned\",\n",
    "#     per_device_train_batch_size=4,\n",
    "#     # gradient_accumulation_steps=2, \n",
    "#     num_train_epochs=1,\n",
    "#     max_steps=20,                      # limit training to 20 steps for estimate\n",
    "#     logging_steps=5,\n",
    "#     fp16=True,\n",
    "#     disable_tqdm=False\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# start = time.time()\n",
    "# trainer.train()\n",
    "# end = time.time()\n",
    "\n",
    "# time_per_step = (end - start) / 20\n",
    "# total_steps = int(len(train_dataset) / 4)  # total steps in 1 epoch\n",
    "# estimated_total = time_per_step * total_steps\n",
    "\n",
    "# print(f\"\\nEstimated total training time: {estimated_total / 60:.2f} minutes\")\n",
    "\n",
    "print('Estimated time ~150 min. \\nCommented and restarted kernel to save CUDA memory \\n(may continue without commenintg for GPU: min 8GB, RTX 3050)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692eb8d5",
   "metadata": {},
   "source": [
    "# Memory handling before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6b5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 2050\n",
      "Compute Capability: (8, 6)\n",
      "FP16 is supported on this GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA capability checking\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_name = torch.cuda.get_device_name(device)\n",
    "    compute_capability = torch.cuda.get_device_capability(device)\n",
    "\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Compute Capability: {compute_capability}\")\n",
    "    \n",
    "\n",
    "    if compute_capability[0] >= 7:      \n",
    "        print(\"FP16 is supported on this GPU.\")\n",
    "    else:\n",
    "        print(\"FP16 is not supported.\")\n",
    "else:\n",
    "    print(\"CUDA not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2ee529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free (available) GPU memory: 2834.45 MB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory after cleanup\n",
    "\n",
    "del wiki_tokenized                # deleting not required variables to save memory\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "free_mem_mb = free_mem / (1024 ** 2) \n",
    "print(f\"Free (available) GPU memory: {free_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77867e6",
   "metadata": {},
   "source": [
    "# Actual model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710d07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 600/3750 [14:41<1:22:53,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 600] Free CUDA memory: 678.45 MB\n",
      "{'loss': 1.2485, 'learning_rate': 4.447887323943662e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1200/3750 [29:37<1:05:55,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1200] Free CUDA memory: 678.45 MB\n",
      "{'loss': 0.8523, 'learning_rate': 3.602816901408451e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1800/3750 [40:20<35:30,  1.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1800] Free CUDA memory: 678.45 MB\n",
      "{'loss': 0.8582, 'learning_rate': 2.7577464788732394e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2400/3750 [50:29<21:42,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2400] Free CUDA memory: 678.45 MB\n",
      "{'loss': 0.7768, 'learning_rate': 1.9126760563380284e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 3000/3750 [1:01:48<12:35,  1.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3000] Free CUDA memory: 678.45 MB\n",
      "{'loss': 0.825, 'learning_rate': 1.067605633802817e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 3600/3750 [1:12:06<02:40,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3600] Free CUDA memory: 678.45 MB\n",
      "{'loss': 0.7731, 'learning_rate': 2.2253521126760562e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 3750/3750 [1:17:42<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3750] Free CUDA memory: 678.45 MB\n",
      "{'eval_loss': 0.8296507000923157, 'eval_runtime': 181.2336, 'eval_samples_per_second': 17.657, 'eval_steps_per_second': 4.414, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:17:52<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3750] Free CUDA memory: 678.45 MB\n",
      "{'train_runtime': 4672.4649, 'train_samples_per_second': 3.21, 'train_steps_per_second': 0.803, 'train_loss': 0.8849034159342448, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.8849034159342448, metrics={'train_runtime': 4672.4649, 'train_samples_per_second': 3.21, 'train_steps_per_second': 0.803, 'train_loss': 0.8849034159342448, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Custom callback to log free CUDA memory\n",
    "class CUDAMemoryLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()                                # CUDA cache clear comes with trade-off > more frequent memory call up increased learning time\n",
    "            free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "            free_mb = free_mem / (1024 ** 2)\n",
    "            print(f\"[Step {state.global_step}] Free CUDA memory: {free_mb:.2f} MB\")\n",
    "\n",
    "# Training parametres\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # gradient_accumulation_steps=2,       #commented to reduce learning time > more preferred over clearing cache\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",           # evaluate every epoch\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=600,                    #checking details after 600 steps  \n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=200,                      # default: lr_scheduler = linear,  ~10% of total steps (1 epoch 15k/4 ~3500 steps)\n",
    "    weight_decay=0.01,                     # default optimizer AdamW, wt decay ~ regularization\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,                     # best two model will be saved after all epochs, bad ones automatically gets deleted to save space when new better version comes\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,               # eval_loss for less it should be less thus 'False'\n",
    "    fp16=True,                             # 16 bit, mixed precision\n",
    "    prediction_loss_only=False,\n",
    "    disable_tqdm=False                     # show tqdm progress bar\n",
    ")\n",
    "\n",
    "# Final Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,          \n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=2),        # early stopping, though uselss for 1 epoch\n",
    "        CUDAMemoryLoggerCallback()                               #custom memory log callback\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf044c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1df06ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [56:44<00:00,  3.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 2.26\n",
      "Top-5 Accuracy: 90.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# fine tuned model loading\n",
    "model_path = \"./gpt2-finetuned/checkpoint-3750\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def evaluate(model, dataset, top_k=5):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "    losses = []\n",
    "    topk_correct = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction=\"none\")\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            losses.append(loss.mean().item())\n",
    "\n",
    "            # Top-k accuracy\n",
    "            topk = torch.topk(shift_logits, k=top_k, dim=-1).indices\n",
    "            match = (topk == shift_labels.unsqueeze(-1)).any(-1)\n",
    "            topk_correct += match.sum().item()\n",
    "            total_preds += match.numel()\n",
    "\n",
    "    perplexity = math.exp(np.mean(losses))\n",
    "    topk_acc = topk_correct / total_preds\n",
    "    return perplexity, topk_acc\n",
    "\n",
    "# evaluation process\n",
    "perplexity, topk_acc = evaluate(model, test_dataset)\n",
    "print(f\"\\nPerplexity: {perplexity:.2f}\")\n",
    "print(f\"Top-{5} Accuracy: {topk_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad389ddd",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13be4adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# fine tuned model \n",
    "model_path = \"./gpt2-finetuned/checkpoint-3750\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "# Prediction function\n",
    "def generate_next_word(prompt, max_length=30):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=inputs.input_ids.shape[1] + max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "# Gradio Interface\n",
    "gr.Interface(\n",
    "    fn=generate_next_word,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter a prompt...\", label=\"Input Prompt\"),\n",
    "        gr.Slider(5, 100, value=30, step=1, label=\"Max Output Length\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"GPT-2 Next Word Prediction\",\n",
    "    description=\"Enter a prompt and see how the model completes it.\",\n",
    "    theme=\"default\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3b1bf",
   "metadata": {},
   "source": [
    "# LSTM comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1dcf9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki    #already loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e20961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# using GPT2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "# encoding texts to match GPT inputs\n",
    "def tokenize_and_pad(texts, max_len=256):\n",
    "    all_ids = []\n",
    "    for t in texts:\n",
    "        if t.strip():\n",
    "            ids = tokenizer(t, truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=None)[\"input_ids\"]\n",
    "            all_ids.extend(ids)\n",
    "    return all_ids\n",
    "\n",
    "#loading dtaset \n",
    "train_data = tokenize_and_pad(wiki[\"train\"][\"text\"])\n",
    "eval_data = tokenize_and_pad(wiki[\"validation\"][\"text\"])\n",
    "test_data  = tokenize_and_pad(wiki[\"test\"][\"text\"])\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 1  # account for added pad_token\n",
    "\n",
    "# Dataset class\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data[idx:idx+self.seq_len]),\n",
    "                torch.tensor(self.data[idx+1:idx+self.seq_len+1]))\n",
    "\n",
    "# hyperparameters\n",
    "SEQ_LEN = 16\n",
    "BATCH_SIZE = 16\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 1\n",
    "PATIENCE = 1\n",
    "TOP_K = 5\n",
    "\n",
    "# Dataloaders\n",
    "dataset = WordDataset(train_data, SEQ_LEN)\n",
    "eval_dataset = WordDataset(eval_data, SEQ_LEN)\n",
    "test_dataset = WordDataset(test_data, SEQ_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# LSTM architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Training setup\n",
    "model = LSTMModel(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Early stopping\n",
    "best_eval_loss = float('inf')\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c3d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                    | 4/154847 [00:06<51:54:15,  1.21s/it, loss=9.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 10.8989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|█████████████▌                     | 60005/154847 [22:22<33:53, 46.64it/s, loss=1.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 60000, Loss: 1.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|████████████████████████▊       | 120008/154847 [1:23:17<12:47, 45.41it/s, loss=1.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 120000, Loss: 1.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████| 154847/154847 [1:36:03<00:00, 26.87it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Training Loss: 1.6883\n",
      "Epoch 1, Validation Loss: 2.6409\n",
      "Epoch 1 completed in 5930.72 seconds\n",
      "\n",
      "Training completed in 98.85 minutes.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "overall_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\", ncols=100)\n",
    "    for step, (x, y) in progress_bar:\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        if step % 60000 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in eval_loader:\n",
    "            x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(eval_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_eval_loss:\n",
    "        best_eval_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining completed in {(time.time() - overall_start_time) / 60:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a13ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42319/42319 [05:42<00:00, 123.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Completed:\n",
      "Perplexity: 13.81\n",
      "Top-5 Accuracy: 75.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model for evaluation\n",
    "\n",
    "# model.load_state_dict(torch.load(\"best_model.pt\")) 1 epoch only used\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Test evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    topk_correct = 0\n",
    "    total_preds = 0\n",
    "    for x, y in tqdm(test_loader):\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        topk = torch.topk(logits, k=TOP_K, dim=-1).indices\n",
    "        match = (topk == y.unsqueeze(-1)).any(-1)\n",
    "        topk_correct += match.sum().item()\n",
    "        total_preds += match.numel()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    topk_acc = topk_correct / total_preds\n",
    "    print(f\"Evaluation Completed:\\nPerplexity: {perplexity:.2f}\\nTop-{TOP_K} Accuracy: {topk_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac849cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT model: \n",
      "Perplexity: 2.26\n",
      "Top-5 Accuracy: 90.53%\n"
     ]
    }
   ],
   "source": [
    "print('GPT model: \\nPerplexity: 2.26\\nTop-5 Accuracy: 90.53%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
